from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.empty import EmptyOperator
from datetime import datetime, timedelta
import requests
import os
import sys
import pandas as pd
from io import StringIO

# Import LoginInformation v√† TaraFunction
current_dir = os.path.dirname(__file__)
project_path = os.path.abspath(os.path.join(current_dir, '../plugins/Project'))
sys.path.append(project_path)

from LoginInformation import Netsuite_getLoginCookies, Netsuite_getLoginAccount
from TaraFunction import upsert_data2datalake

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2024, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    dag_id='netsuite_saved_search_4532_memory',
    default_args=default_args,
    schedule_interval='@daily',
    catchup=False,
    tags=['netsuite', 'datalake', 'memory']
) as dag:

    start = EmptyOperator(task_id='start')

    def extract_from_netsuite(**context):
        searchid = '4532'
        login_cookies = Netsuite_getLoginCookies()
        login_data = Netsuite_getLoginAccount()

        # Debug login data v√† cookies
        print("üîê login_data:", login_data)
        print("üç™ login_cookies:", login_cookies)

        auth_url = 'https://7258775.app.netsuite.com/app/login/secure/enterpriselogin.nl'
        download_url = (
            f"https://7258775.app.netsuite.com/app/common/search/searchresults.csv?"
            f"searchid={searchid}&frame=be&csv=Export&report=&grid=T"
            f"&Transaction_TRANDATEfrom=01%2F01%2F2024"
        )

        session = requests.Session()
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
            "Referer": "https://7258775.app.netsuite.com",
            "Origin": "https://7258775.app.netsuite.com"
        }

        login_response = session.post(auth_url, data=login_data, cookies=login_cookies, headers=headers)

        # N·∫øu tr·∫£ v·ªÅ HTML (login page), raise l·ªói
        if login_response.status_code != 200 or "<html" in login_response.text.lower():
            print("üîç Login status code:", login_response.status_code)
            print("üîç Content-Type:", login_response.headers.get("Content-Type", "N/A"))
            print("üîç Response sample:", login_response.text[:300])
            raise Exception("‚ùå ƒêƒÉng nh·∫≠p NetSuite th·∫•t b·∫°i ho·∫∑c tr·∫£ v·ªÅ HTML")

        response = session.get(download_url, verify=True, timeout=60)
        csv_text = response.text.strip()

        if response.status_code != 200 or not csv_text or "<html" in csv_text.lower():
            print("‚ö†Ô∏è CSV response b·ªã l·ªói ho·∫∑c l√† HTML:")
            print("üîç Status:", response.status_code)
            print("üîç CSV preview:", csv_text[:300])
            raise Exception("‚ùå D·ªØ li·ªáu NetSuite tr·∫£ v·ªÅ kh√¥ng h·ª£p l·ªá")

        # Parse CSV trong memory
        df = pd.read_csv(StringIO(csv_text))
        df['bi_updatetime'] = datetime.today().replace(microsecond=0)
        df['source'] = 'netsuite_4532'

        # ƒê·∫©y JSON h√≥a df v√†o XCom
        context['ti'].xcom_push(key='df_csv', value=df.to_json(orient='records'))
        print(f"‚úÖ L·∫•y {len(df)} d√≤ng d·ªØ li·ªáu t·ª´ NetSuite")

    def load_to_datalake(**context):
        df_json = context['ti'].xcom_pull(key='df_csv')
        df = pd.read_json(df_json, orient='records')

        upsert_data2datalake(
            df=df,
            tablename='"NetSuite"."F99_CustomSearch4532"',
            primary_key='source'
        )
        print("‚úÖ ƒê·∫©y d·ªØ li·ªáu v√†o Datalake th√†nh c√¥ng")

    extract = PythonOperator(
        task_id='extract_from_netsuite',
        python_callable=extract_from_netsuite,
        provide_context=True
    )

    load = PythonOperator(
        task_id='load_to_datalake',
        python_callable=load_to_datalake,
        provide_context=True
    )

    end = EmptyOperator(task_id='end')

    start >> extract >> load >> end
